{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347692d1",
   "metadata": {},
   "source": [
    "# 1. RNN을 이용하여 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4faab",
   "metadata": {},
   "source": [
    "## 1) 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c7773d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "409c5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d2f6802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'경마장에 있는 말이 뛰고 있다\\n\\n그의 말이 법이다\\n\\n가는 말이 고와야 오는 말이 곱다\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99e855c4",
   "metadata": {},
   "source": [
    "단어 집합을 생성하고 크기를 확인해보겠습니다. \n",
    "단어 집합의 크기를 저장할 때는 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만, \n",
    "패딩을 위한 0을 고려하여 +1을 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1a71a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'말이': 1,\n",
       " '경마장에': 2,\n",
       " '있는': 3,\n",
       " '뛰고': 4,\n",
       " '있다': 5,\n",
       " '그의': 6,\n",
       " '법이다': 7,\n",
       " '가는': 8,\n",
       " '고와야': 9,\n",
       " '오는': 10,\n",
       " '곱다': 11}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts([text])\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "30a5562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n",
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "# 패딩을 위한 0을 고려하여 +1을 해줍니다.\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "761fa37f",
   "metadata": {},
   "source": [
    "훈련 데이터를 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5162248b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 사용할 샘플의 개수: 11\n",
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): # 줄바꿈 문자를 기준으로 문장 토큰화\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
    "print(sequences)  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "68ec97e7",
   "metadata": {},
   "source": [
    "위의 데이터는 아직 레이블로 사용될 단어를 분리하지 않은 훈련 데이터입니다. [2, 3]은 [경마장에, 있는]에 해당되며 [2, 3, 1]은 [경마장에, 있는, 말이]에 해당됩니다. 전체 훈련 데이터에 대해서 맨 우측에 있는 단어에 대해서만 레이블로 분리해야 합니다.\n",
    "\n",
    "패딩\n",
    "우선 전체 샘플에 대해서 길이를 일치시켜 줍니다. 가장 긴 샘플의 길이를 기준으로 합니다. \n",
    "현재 육안으로 봤을 때, 길이가 가장 긴 샘플은 [8, 1, 9, 10, 1, 11]이고 길이는 6입니다. 이를 코드로는 다음과 같이 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "77376f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 6\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e531766",
   "metadata": {},
   "source": [
    "전체 훈련 데이터에서 가장 긴 샘플의 길이가 6임을 확인하였습니다. 전체 샘플의 길이를 6으로 패딩합니다.\n",
    "pad_sequences()는 모든 샘플에 대해서 0을 사용하여 길이를 맞춰줍니다. maxlen의 값으로 6을 주면 모든 샘플의 길이를 6으로 맞춰주며, padding의 인자로 'pre'를 주면 길이가 6보다 짧은 샘플의 앞에 0으로 채웁니다. 전체 훈련 데이터를 출력해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "870e6c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  2,  3,  1],\n",
       "       [ 0,  0,  2,  3,  1,  4],\n",
       "       [ 0,  2,  3,  1,  4,  5],\n",
       "       [ 0,  0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  6,  1,  7],\n",
       "       [ 0,  0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  0,  8,  1,  9],\n",
       "       [ 0,  0,  8,  1,  9, 10],\n",
       "       [ 0,  8,  1,  9, 10,  1],\n",
       "       [ 8,  1,  9, 10,  1, 11]], dtype=int32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "sequences"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cfb8ef6",
   "metadata": {},
   "source": [
    "길이가 6보다 짧은 모든 샘플에 대해서 앞에 0을 채워서 모든 샘플의 길이를 6으로 바꿨습니다. \n",
    "이제 각 샘플의 마지막 단어를 레이블로 분리합시다. 레이블의 분리는 Numpy를 이용해서 가능합니다. \n",
    "리스트의 마지막 값을 제외하고 저장한 것은 X, 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1891848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7cf639ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  2  3  1]\n",
      " [ 0  2  3  1  4]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  6  1]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  8  1]\n",
      " [ 0  0  8  1  9]\n",
      " [ 0  8  1  9 10]\n",
      " [ 8  1  9 10  1]]\n",
      "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fce8714",
   "metadata": {},
   "source": [
    "레이블이 분리되었습니다. \n",
    "RNN 모델에 훈련 데이터를 훈련 시키기 전에 레이블에 대해서 원-핫 인코딩을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0db56331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cc69e0e",
   "metadata": {},
   "source": [
    "-> 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4be8b7",
   "metadata": {},
   "source": [
    "## 2) RNN 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2625bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf171ca9",
   "metadata": {},
   "source": [
    "하이퍼 파라미터인 임베딩 벡터의 차원은 10, 은닉 상태의 크기는 32입니다. 다 대 일 구조의 RNN을 사용합니다. \n",
    "주어진 단어 집합의 크기는 12입니다.\n",
    "전결합층(Fully Connected Layer)을 출력층으로 단어 집합 크기만큼의 뉴런을 배치하여 모델을 설계합니다. \n",
    "\n",
    "해당 모델은 마지막 시점에서 모든 가능한 단어 중 하나의 단어를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. \n",
    "다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 200 에포크를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cf530853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합의 크기\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af6c4b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f751696795b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhidden_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# 단어 집합의 크기는 12. 임베딩 벡터의 크기는 10. 각 sample의 길이는 단어 5개이므로 길이는 5.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_dim = 10\n",
    "hidden_units = 32\n",
    "\n",
    "model = Sequential()\n",
    "# 단어 집합의 크기는 12. 임베딩 벡터의 크기는 10. 각 sample의 길이는 단어 5개이므로 길이는 5.\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=5))\n",
    "# RNN의 결과값으로 나오는 벡터의 차원은 32로 한다. 더 크게 해주어도 상관은 없음.\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X, y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c926508",
   "metadata": {},
   "source": [
    "모델이 정확하게 예측하고 있는지 문장을 생성하는 함수를 만들어서 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e85422b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력된 단어로부터 다음 단어를 예측해서 문장을 생성하는 함수. \n",
    "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "\n",
    "    # n번 반복\n",
    "    for _ in range(n):\n",
    "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
    "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "\n",
    "        for word, index in tokenizer.word_index.items(): \n",
    "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
    "            if index == result:\n",
    "                break\n",
    "\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' '  + word\n",
    "\n",
    "        # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "raw",
   "id": "632aa6af",
   "metadata": {},
   "source": [
    "'경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "295ad13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '경마장에', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4422b925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그의 말이 법이다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '그의', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0cf3e914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, tokenizer, '가는', 5))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adff4f22",
   "metadata": {},
   "source": [
    "앞의 문맥을 기준으로 '말이' 라는 단어 다음에 나올 단어를 기존의 훈련 데이터와 일치하게 예측함을 보여줍니다. \n",
    "이 모델은 충분한 훈련 데이터를 갖고 있지 못하므로 위에서 문장의 길이에 맞게 적절하게 예측해야하는 횟수 4, 2, 5를 각각 인자값으로 주었습니다. \n",
    "이 이상의 숫자를 주면 기계는 '있다', '법이다', '곱다' 다음에 나오는 단어가 무엇인지 배운 적이 없으므로 임의 예측을 합니다. \n",
    "이번에는 더 많은 훈련 데이터를 가지고 실습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b82b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "971f249d",
   "metadata": {},
   "source": [
    "# 2. LSTM을 이용하여 텍스트 생성하기"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d5a5dfd",
   "metadata": {},
   "source": [
    "이번에는 LSTM을 통해 보다 많은 데이터로 텍스트를 생성해보겠습니다. 본질적으로 앞에서 한 것과 동일한 실습입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a5945",
   "metadata": {},
   "source": [
    "## 1) 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ced054b",
   "metadata": {},
   "source": [
    "사용할 데이터는 뉴욕 타임즈 기사의 제목입니다. 아래의 링크에서 ArticlesApril2018.csv 데이터를 다운로드 합니다.\n",
    "파일 다운로드 링크 : https://www.kaggle.com/aashita/nyt-comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8abaa",
   "metadata": {},
   "source": [
    "### 데이터 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b104b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c807cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1324, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781</td>\n",
       "      <td>By JOHN BRANCH</td>\n",
       "      <td>article</td>\n",
       "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
       "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
       "      <td>68</td>\n",
       "      <td>Sports</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:16:49</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>“I understand that they could meet with us, pa...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656</td>\n",
       "      <td>By LISA FRIEDMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
       "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
       "      <td>68</td>\n",
       "      <td>Climate</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 17:11:21</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>The agency plans to publish a new regulation T...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5adf4626068401528a2aa628</td>\n",
       "      <td>2427</td>\n",
       "      <td>By PETE WELLS</td>\n",
       "      <td>article</td>\n",
       "      <td>The New Noma, Explained</td>\n",
       "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
       "      <td>66</td>\n",
       "      <td>Dining</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:58:44</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>What’s it like to eat at the second incarnatio...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5adf40d2068401528a2aa619</td>\n",
       "      <td>626</td>\n",
       "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
       "      <td>68</td>\n",
       "      <td>Washington</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:35:57</td>\n",
       "      <td>Europe</td>\n",
       "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5adf3d64068401528a2aa60f</td>\n",
       "      <td>815</td>\n",
       "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
       "      <td>article</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
       "      <td>68</td>\n",
       "      <td>Foreign</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-04-24 14:21:21</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  articleID  articleWordCount  \\\n",
       "0  5adf6684068401528a2aa69b               781   \n",
       "1  5adf653f068401528a2aa697               656   \n",
       "2  5adf4626068401528a2aa628              2427   \n",
       "3  5adf40d2068401528a2aa619               626   \n",
       "4  5adf3d64068401528a2aa60f               815   \n",
       "\n",
       "                                      byline documentType  \\\n",
       "0                             By JOHN BRANCH      article   \n",
       "1                           By LISA FRIEDMAN      article   \n",
       "2                              By PETE WELLS      article   \n",
       "3  By JULIE HIRSCHFELD DAVIS and PETER BAKER      article   \n",
       "4             By IAN AUSTEN and DAN BILEFSKY      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Former N.F.L. Cheerleaders’ Settlement Offer: ...   \n",
       "1  E.P.A. to Unveil a New Rule. Its Effect: Less ...   \n",
       "2                            The New Noma, Explained   \n",
       "3                                            Unknown   \n",
       "4                                            Unknown   \n",
       "\n",
       "                                            keywords  multimedia     newDesk  \\\n",
       "0  ['Workplace Hazards and Violations', 'Football...          68      Sports   \n",
       "1  ['Environmental Protection Agency', 'Pruitt, S...          68     Climate   \n",
       "2  ['Restaurants', 'Noma (Copenhagen, Restaurant)...          66      Dining   \n",
       "3  ['Macron, Emmanuel (1977- )', 'Trump, Donald J...          68  Washington   \n",
       "4  ['Toronto, Ontario, Attack (April, 2018)', 'Mu...          68     Foreign   \n",
       "\n",
       "   printPage              pubDate   sectionName  \\\n",
       "0          0  2018-04-24 17:16:49  Pro Football   \n",
       "1          0  2018-04-24 17:11:21       Unknown   \n",
       "2          0  2018-04-24 14:58:44       Unknown   \n",
       "3          0  2018-04-24 14:35:57        Europe   \n",
       "4          0  2018-04-24 14:21:21        Canada   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  “I understand that they could meet with us, pa...  The New York Times   \n",
       "1  The agency plans to publish a new regulation T...  The New York Times   \n",
       "2  What’s it like to eat at the second incarnatio...  The New York Times   \n",
       "3  President Trump welcomed President Emmanuel Ma...  The New York Times   \n",
       "4  Alek Minassian, 25, a resident of Toronto’s Ri...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2018/04/24/sports/foot...  \n",
       "1           News  https://www.nytimes.com/2018/04/24/climate/epa...  \n",
       "2           News  https://www.nytimes.com/2018/04/24/dining/noma...  \n",
       "3           News  https://www.nytimes.com/2018/04/24/world/europ...  \n",
       "4           News  https://www.nytimes.com/2018/04/24/world/canad...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_articles/ArticlesApril2018.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f27d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "열의 개수:  15\n",
      "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
      "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
      "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('열의 개수: ', len(df.columns))\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03bea6ba",
   "metadata": {},
   "source": [
    "총 15개의 열이 존재합니다. 여기서 사용할 열은 제목에 해당되는 headline 열입니다. Null 값이 있는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22cb41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Null 값은 별도로 없는 것으로 보입니다. \n",
    "print(df['headline'].isnull().values.any())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb9a2a0e",
   "metadata": {},
   "source": [
    "headline 열에서 모든 신문 기사의 제목을 뽑아서 하나의 리스트로 저장해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24ce1b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'Unknown',\n",
       " 'Unknown']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline = []\n",
    "# 헤드라인의 값들을 리스트로 저장\n",
    "headline.extend(list(df.headline.values))\n",
    "headline[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14e9245b",
   "metadata": {},
   "source": [
    "네번째와 다섯 번째 샘플에 Unknown 값이 들어가있습니다. headline 전체에 걸쳐서 Unknown 값을 가진 샘플이 있을 것으로 추정됩니다. \n",
    "비록 Null 값은 아니지만 실습에 도움이 되지 않는 노이즈 데이터이므로 제거해줄 필요가 있습니다. \n",
    "제거하기 전에 현재 샘플의 개수를 확인해보고 제거 전, 후의 샘플의 개수를 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f581ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플의 개수 : 1324\n"
     ]
    }
   ],
   "source": [
    "print('총 샘플의 개수 : {}'.format(len(headline)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "976150ba",
   "metadata": {},
   "source": [
    "노이즈 데이터를 제거하기 전 신문 기사의 제목 샘플은 총 1,324개입니다. Unknown 값을 가진 샘플을 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7e53b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "노이즈값 제거 후 샘플의 개수 : 1214\n"
     ]
    }
   ],
   "source": [
    "headline = [word for word in headline if word != \"Unknown\"]\n",
    "print('노이즈값 제거 후 샘플의 개수 : {}'.format(len(headline)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4150bd1",
   "metadata": {},
   "source": [
    "샘플의 수가 1,324에서 1,214로 110개의 샘플이 제거되었는데 기존에 출력했던 5개의 샘플을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d5cc4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
       " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
       " 'The New Noma, Explained',\n",
       " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
       " 'Is School a Place for Self-Expression?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34834fd9",
   "metadata": {},
   "source": [
    "기존에 네번째, 다섯 번째 샘플에서는 Unknown 값이 있었는데 현재는 제거가 되었습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fddd3",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d5557b0",
   "metadata": {},
   "source": [
    "이제 데이터 전처리를 수행합니다. 여기서 선택한 전처리는 구두점 제거와 단어의 소문자화입니다. 전처리를 수행하고, 다시 샘플 5개를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9811fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
       " 'epa to unveil a new rule its effect less science in policymaking',\n",
       " 'the new noma explained',\n",
       " 'how a bag of texas dirt  became a times tradition',\n",
       " 'is school a place for selfexpression']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def repreprocessing(raw_sentence):\n",
    "    preproceseed_sentence = raw_sentence.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    # 구두점 제거와 동시에 소문자화\n",
    "    return ''.join(word for word in preproceseed_sentence if word not in punctuation).lower()\n",
    "\n",
    "preprocessed_headline = [repreprocessing(x) for x in headline]\n",
    "preprocessed_headline[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "785263ac",
   "metadata": {},
   "source": [
    "기존의 출력과 비교하면 모든 단어들이 소문자화되었으며 N.F.L.이나 Cheerleaders’ 등과 같이 기존에 구두점이 붙어있던 단어들에서 구두점이 제거되었습니다. 이제 단어 집합(vocabulary)을 만들고 크기를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d87b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 3494\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_headline)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('단어 집합의 크기 : %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b7c1d78",
   "metadata": {},
   "source": [
    "총 3,494개의 단어가 존재합니다. 정수 인코딩을 진행하는 동시에 하나의 문장을 여러 줄로 분해하여 훈련 데이터를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "670df998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[99, 269],\n",
       " [99, 269, 371],\n",
       " [99, 269, 371, 1115],\n",
       " [99, 269, 371, 1115, 582],\n",
       " [99, 269, 371, 1115, 582, 52],\n",
       " [99, 269, 371, 1115, 582, 52, 7],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
       " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
       " [100, 3]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = list()\n",
    "\n",
    "for sentence in preprocessed_headline:\n",
    "\n",
    "    # 각 샘플에 대한 정수 인코딩\n",
    "    encoded = tokenizer.texts_to_sequences([sentence])[0] \n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "\n",
    "sequences[:11]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf50d33c",
   "metadata": {},
   "source": [
    "[[99, 269], # former nfl\n",
    " [99, 269, 371], # former nfl cheerleaders\n",
    " [99, 269, 371, 1115], # former nfl cheerleaders settlement\n",
    " [99, 269, 371, 1115, 582], # former nfl cheerleaders settlement offer\n",
    " [99, 269, 371, 1115, 582, 52], # 'former nfl cheerleaders settlement offer 1\n",
    " [99, 269, 371, 1115, 582, 52, 7], # former nfl cheerleaders settlement offer 1 and\n",
    " [99, 269, 371, 1115, 582, 52, 7, 2], \n",
    " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
    " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
    " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116], # 모든 단어가 사용된 완전한 첫번째 문장\n",
    " # 바로 위의 줄은 : former nfl cheerleaders settlement offer 1 and a meeting with goodell\n",
    " [100, 3]] # epa to에 해당되며 두번째 문장이 시작됨.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24dab856",
   "metadata": {},
   "source": [
    "이해를 돕기 위해 출력 결과에 주석을 추가하였습니다. \n",
    "왜 하나의 문장을 저렇게 나눌까요? 예를 들어 '경마장에 있는 말이 뛰고 있다' 라는 문장 하나가 있을 때, 최종적으로 원하는 훈련 데이터의 형태는 다음과 같습니다. 하나의 단어를 예측하기 위해 이전에 등장한 단어들을 모두 참고하는 것입니다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f06c4473",
   "metadata": {},
   "source": [
    "위의 sequences는 모든 문장을 각 단어가 각 시점(time step)마다 하나씩 추가적으로 등장하는 형태로 만들기는 했지만, 아직 예측할 단어에 해당되는 레이블을 분리하는 작업까지는 수행하지 않은 상태입니다. 어떤 정수가 어떤 단어를 의미하는지 알아보기 위해 인덱스로부터 단어를 찾는 index_to_word를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c363da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'a',\n",
       " 3: 'to',\n",
       " 4: 'of',\n",
       " 5: 'in',\n",
       " 6: 'for',\n",
       " 7: 'and',\n",
       " 8: 'is',\n",
       " 9: 'on',\n",
       " 10: 'with',\n",
       " 11: 'trump',\n",
       " 12: 'as',\n",
       " 13: 'at',\n",
       " 14: 'new',\n",
       " 15: 'how',\n",
       " 16: 'from',\n",
       " 17: 'it',\n",
       " 18: 'an',\n",
       " 19: 'that',\n",
       " 20: 'be',\n",
       " 21: 'season',\n",
       " 22: 'us',\n",
       " 23: 'you',\n",
       " 24: 'its',\n",
       " 25: 'what',\n",
       " 26: 'episode',\n",
       " 27: 'can',\n",
       " 28: 'your',\n",
       " 29: 'not',\n",
       " 30: 'he',\n",
       " 31: 'now',\n",
       " 32: 'his',\n",
       " 33: 'are',\n",
       " 34: 'teaching',\n",
       " 35: 'war',\n",
       " 36: 'out',\n",
       " 37: 'no',\n",
       " 38: 'was',\n",
       " 39: 'by',\n",
       " 40: 'trumps',\n",
       " 41: 'has',\n",
       " 42: 'over',\n",
       " 43: 'may',\n",
       " 44: 'into',\n",
       " 45: 'why',\n",
       " 46: 'more',\n",
       " 47: 'we',\n",
       " 48: 'who',\n",
       " 49: 'about',\n",
       " 50: 'recap',\n",
       " 51: 'activities',\n",
       " 52: '1',\n",
       " 53: 'just',\n",
       " 54: 'do',\n",
       " 55: 'women',\n",
       " 56: 'when',\n",
       " 57: 'syria',\n",
       " 58: 'trade',\n",
       " 59: 'i',\n",
       " 60: '2',\n",
       " 61: 'or',\n",
       " 62: 'will',\n",
       " 63: 'this',\n",
       " 64: 'have',\n",
       " 65: 'president',\n",
       " 66: 'but',\n",
       " 67: 'home',\n",
       " 68: 'up',\n",
       " 69: 'long',\n",
       " 70: 'one',\n",
       " 71: 'off',\n",
       " 72: 'facebook',\n",
       " 73: 'house',\n",
       " 74: 'gop',\n",
       " 75: 'our',\n",
       " 76: 'case',\n",
       " 77: 'they',\n",
       " 78: 'life',\n",
       " 79: 'end',\n",
       " 80: 'right',\n",
       " 81: 'some',\n",
       " 82: 'big',\n",
       " 83: 'dead',\n",
       " 84: 'power',\n",
       " 85: 'say',\n",
       " 86: 'white',\n",
       " 87: 'after',\n",
       " 88: 'still',\n",
       " 89: 'north',\n",
       " 90: 'my',\n",
       " 91: 'dont',\n",
       " 92: 'need',\n",
       " 93: 'race',\n",
       " 94: 'own',\n",
       " 95: 'against',\n",
       " 96: 'here',\n",
       " 97: 'should',\n",
       " 98: 'border',\n",
       " 99: 'former',\n",
       " 100: 'epa',\n",
       " 101: 'battle',\n",
       " 102: 'mr',\n",
       " 103: 'too',\n",
       " 104: 'their',\n",
       " 105: 'plan',\n",
       " 106: '3',\n",
       " 107: 'china',\n",
       " 108: 'real',\n",
       " 109: 'were',\n",
       " 110: 'her',\n",
       " 111: 'russia',\n",
       " 112: 'art',\n",
       " 113: 'good',\n",
       " 114: 'then',\n",
       " 115: 'like',\n",
       " 116: 'pay',\n",
       " 117: 'back',\n",
       " 118: 'get',\n",
       " 119: 'love',\n",
       " 120: 'says',\n",
       " 121: 'officials',\n",
       " 122: 'fight',\n",
       " 123: 'tariffs',\n",
       " 124: 'pruitt',\n",
       " 125: 'democrats',\n",
       " 126: 'black',\n",
       " 127: 'man',\n",
       " 128: 'men',\n",
       " 129: 'help',\n",
       " 130: 'never',\n",
       " 131: 'york',\n",
       " 132: 'comey',\n",
       " 133: 'chief',\n",
       " 134: 'metoo',\n",
       " 135: 'work',\n",
       " 136: 'place',\n",
       " 137: 'could',\n",
       " 138: 'past',\n",
       " 139: 'years',\n",
       " 140: 'rights',\n",
       " 141: 'first',\n",
       " 142: 'money',\n",
       " 143: 'save',\n",
       " 144: 'going',\n",
       " 145: 'all',\n",
       " 146: 'way',\n",
       " 147: 'political',\n",
       " 148: 'fear',\n",
       " 149: 'next',\n",
       " 150: 'fire',\n",
       " 151: 'party',\n",
       " 152: 'me',\n",
       " 153: 'becomes',\n",
       " 154: '8',\n",
       " 155: 'better',\n",
       " 156: 'old',\n",
       " 157: 'dr',\n",
       " 158: 'king',\n",
       " 159: 'homes',\n",
       " 160: 'ryan',\n",
       " 161: 'tax',\n",
       " 162: 'if',\n",
       " 163: 'than',\n",
       " 164: 'americans',\n",
       " 165: 'rules',\n",
       " 166: 'police',\n",
       " 167: 'school',\n",
       " 168: 'leaders',\n",
       " 169: 'korea',\n",
       " 170: 'there',\n",
       " 171: 'top',\n",
       " 172: 'court',\n",
       " 173: 'state',\n",
       " 174: '10',\n",
       " 175: 'lower',\n",
       " 176: 'states',\n",
       " 177: 'whats',\n",
       " 178: 'use',\n",
       " 179: 'cancer',\n",
       " 180: 'britain',\n",
       " 181: 'wont',\n",
       " 182: 'time',\n",
       " 183: 'america',\n",
       " 184: 'history',\n",
       " 185: 'where',\n",
       " 186: 'lead',\n",
       " 187: 'left',\n",
       " 188: 'plans',\n",
       " 189: 'talk',\n",
       " 190: 'crisis',\n",
       " 191: 'two',\n",
       " 192: 'tells',\n",
       " 193: 'trust',\n",
       " 194: 'nuclear',\n",
       " 195: 'children',\n",
       " 196: 'million',\n",
       " 197: 'make',\n",
       " 198: 'leader',\n",
       " 199: 'others',\n",
       " 200: 'attack',\n",
       " 201: 'people',\n",
       " 202: 'another',\n",
       " 203: '6',\n",
       " 204: 'world',\n",
       " 205: 'day',\n",
       " 206: 'car',\n",
       " 207: 'young',\n",
       " 208: 'little',\n",
       " 209: 'california',\n",
       " 210: 'crash',\n",
       " 211: 'last',\n",
       " 212: 'book',\n",
       " 213: 'death',\n",
       " 214: 'gun',\n",
       " 215: 'texas',\n",
       " 216: 'hes',\n",
       " 217: 'sex',\n",
       " 218: 'abuse',\n",
       " 219: 'find',\n",
       " 220: 'truth',\n",
       " 221: 'arizona',\n",
       " 222: 'american',\n",
       " 223: 'pompeo',\n",
       " 224: 'change',\n",
       " 225: 'city',\n",
       " 226: 'kim',\n",
       " 227: 'so',\n",
       " 228: 'music',\n",
       " 229: 'risks',\n",
       " 230: 'being',\n",
       " 231: 'billions',\n",
       " 232: '5',\n",
       " 233: 'family',\n",
       " 234: 'missing',\n",
       " 235: 'gaza',\n",
       " 236: 'vs',\n",
       " 237: 'senate',\n",
       " 238: 'heart',\n",
       " 239: 'justice',\n",
       " 240: 'killing',\n",
       " 241: 'cia',\n",
       " 242: 'come',\n",
       " 243: 'shows',\n",
       " 244: 'turning',\n",
       " 245: 'schools',\n",
       " 246: 'parents',\n",
       " 247: 'law',\n",
       " 248: 'legal',\n",
       " 249: 'economy',\n",
       " 250: 'strike',\n",
       " 251: 'him',\n",
       " 252: 'threat',\n",
       " 253: 'before',\n",
       " 254: 'takes',\n",
       " 255: 'night',\n",
       " 256: 'mueller',\n",
       " 257: 'close',\n",
       " 258: 'cut',\n",
       " 259: 'dream',\n",
       " 260: 'face',\n",
       " 261: 'friends',\n",
       " 262: 'does',\n",
       " 263: 'game',\n",
       " 264: 'dear',\n",
       " 265: 'boss',\n",
       " 266: 'baby',\n",
       " 267: 'kings',\n",
       " 268: 'jail',\n",
       " 269: 'nfl',\n",
       " 270: 'jimmy',\n",
       " 271: 'word',\n",
       " 272: 'hot',\n",
       " 273: 'turns',\n",
       " 274: 'gap',\n",
       " 275: 'got',\n",
       " 276: 'hope',\n",
       " 277: 'making',\n",
       " 278: 'overlooked',\n",
       " 279: 'push',\n",
       " 280: 'high',\n",
       " 281: 'deal',\n",
       " 282: 'heck',\n",
       " 283: 'live',\n",
       " 284: 'families',\n",
       " 285: 'wrong',\n",
       " 286: '2018',\n",
       " 287: 'fix',\n",
       " 288: 'lawyers',\n",
       " 289: 'koreas',\n",
       " 290: 'choose',\n",
       " 291: 'public',\n",
       " 292: 'cuomo',\n",
       " 293: 'steel',\n",
       " 294: 'open',\n",
       " 295: 'scott',\n",
       " 296: 'beyond',\n",
       " 297: 'variety',\n",
       " 298: 'ethics',\n",
       " 299: 'files',\n",
       " 300: 'dept',\n",
       " 301: 'let',\n",
       " 302: 'inside',\n",
       " 303: 'director',\n",
       " 304: 'far',\n",
       " 305: 'side',\n",
       " 306: 'rupauls',\n",
       " 307: 'drag',\n",
       " 308: 'fence',\n",
       " 309: 'sanctions',\n",
       " 310: 'want',\n",
       " 311: 'global',\n",
       " 312: 'great',\n",
       " 313: 'fish',\n",
       " 314: 'market',\n",
       " 315: 'team',\n",
       " 316: 'eat',\n",
       " 317: 'problem',\n",
       " 318: 'miracle',\n",
       " 319: 'tied',\n",
       " 320: 'hours',\n",
       " 321: 'working',\n",
       " 322: 'other',\n",
       " 323: 'yet',\n",
       " 324: 'call',\n",
       " 325: 'allies',\n",
       " 326: 'privacy',\n",
       " 327: 'data',\n",
       " 328: 'experts',\n",
       " 329: 'go',\n",
       " 330: 'met',\n",
       " 331: 'brooklyn',\n",
       " 332: 'didnt',\n",
       " 333: 'russian',\n",
       " 334: 'trial',\n",
       " 335: 'apply',\n",
       " 336: 'college',\n",
       " 337: 'many',\n",
       " 338: '4',\n",
       " 339: 'security',\n",
       " 340: 'year',\n",
       " 341: 'tale',\n",
       " 342: 'social',\n",
       " 343: 'control',\n",
       " 344: 'been',\n",
       " 345: 'hit',\n",
       " 346: 'early',\n",
       " 347: 'behind',\n",
       " 348: 'match',\n",
       " 349: 'ok',\n",
       " 350: 'fears',\n",
       " 351: 'isis',\n",
       " 352: 'walking',\n",
       " 353: 'national',\n",
       " 354: 'presidency',\n",
       " 355: 'student',\n",
       " 356: 'second',\n",
       " 357: 'limits',\n",
       " 358: 'care',\n",
       " 359: 'era',\n",
       " 360: 'south',\n",
       " 361: 'guard',\n",
       " 362: 'rise',\n",
       " 363: 'edge',\n",
       " 364: 'hero',\n",
       " 365: 'tech',\n",
       " 366: 'secret',\n",
       " 367: 'storm',\n",
       " 368: 'gets',\n",
       " 369: 'watch',\n",
       " 370: 'weapons',\n",
       " 371: 'cheerleaders',\n",
       " 372: 'meeting',\n",
       " 373: 'less',\n",
       " 374: 'science',\n",
       " 375: 'dirt',\n",
       " 376: 'times',\n",
       " 377: 'looking',\n",
       " 378: 'win',\n",
       " 379: 'pope',\n",
       " 380: 'stuff',\n",
       " 381: 'wants',\n",
       " 382: 'ruling',\n",
       " 383: 'town',\n",
       " 384: 'cold',\n",
       " 385: 'behavior',\n",
       " 386: 'guns',\n",
       " 387: 'stand',\n",
       " 388: 'human',\n",
       " 389: 'economic',\n",
       " 390: 'tragedy',\n",
       " 391: 'paul',\n",
       " 392: 'them',\n",
       " 393: 'key',\n",
       " 394: 'down',\n",
       " 395: 'given',\n",
       " 396: 'urge',\n",
       " 397: 'kids',\n",
       " 398: 'westworld',\n",
       " 399: 'picture',\n",
       " 400: 'april',\n",
       " 401: '23',\n",
       " 402: 'subway',\n",
       " 403: 'lets',\n",
       " 404: 'impeachment',\n",
       " 405: 'feel',\n",
       " 406: 'told',\n",
       " 407: 'smile',\n",
       " 408: 'meet',\n",
       " 409: 'wild',\n",
       " 410: 'guide',\n",
       " 411: 'business',\n",
       " 412: 'republicans',\n",
       " 413: 'starbucks',\n",
       " 414: 'door',\n",
       " 415: 'heres',\n",
       " 416: 'punch',\n",
       " 417: 'air',\n",
       " 418: 'caution',\n",
       " 419: 'democratic',\n",
       " 420: 'seen',\n",
       " 421: 'west',\n",
       " 422: 'once',\n",
       " 423: 'marriage',\n",
       " 424: 'politics',\n",
       " 425: 'mission',\n",
       " 426: 'support',\n",
       " 427: 'extra',\n",
       " 428: 'stephen',\n",
       " 429: 'colbert',\n",
       " 430: 'doesnt',\n",
       " 431: 'marijuana',\n",
       " 432: 'reading',\n",
       " 433: 'flight',\n",
       " 434: 'navy',\n",
       " 435: 'veteran',\n",
       " 436: 'ban',\n",
       " 437: 'atlanta',\n",
       " 438: 'finally',\n",
       " 439: 'ready',\n",
       " 440: 'todays',\n",
       " 441: 'puzzle',\n",
       " 442: 'deputy',\n",
       " 443: 'gave',\n",
       " 444: 'teenagers',\n",
       " 445: 'training',\n",
       " 446: 'cant',\n",
       " 447: 'remember',\n",
       " 448: '36',\n",
       " 449: 'found',\n",
       " 450: 'look',\n",
       " 451: 'scientists',\n",
       " 452: 'drug',\n",
       " 453: 'start',\n",
       " 454: 'fit',\n",
       " 455: 'syrian',\n",
       " 456: 'israel',\n",
       " 457: 'bush',\n",
       " 458: 'tweets',\n",
       " 459: 'calling',\n",
       " 460: 'genius',\n",
       " 461: 'acrostic',\n",
       " 462: 'strikes',\n",
       " 463: 'americas',\n",
       " 464: 'pregnancy',\n",
       " 465: 'step',\n",
       " 466: 'washington',\n",
       " 467: 'body',\n",
       " 468: 'supreme',\n",
       " 469: 'best',\n",
       " 470: 'run',\n",
       " 471: 'late',\n",
       " 472: 'force',\n",
       " 473: 'france',\n",
       " 474: 'said',\n",
       " 475: 'raid',\n",
       " 476: 'report',\n",
       " 477: 'aide',\n",
       " 478: 'try',\n",
       " 479: 'line',\n",
       " 480: 'loss',\n",
       " 481: 'risk',\n",
       " 482: 'ask',\n",
       " 483: 'trevor',\n",
       " 484: 'noah',\n",
       " 485: 'become',\n",
       " 486: 'affair',\n",
       " 487: 'died',\n",
       " 488: 'travel',\n",
       " 489: 'pollution',\n",
       " 490: 'building',\n",
       " 491: 'details',\n",
       " 492: 'mike',\n",
       " 493: 'immigration',\n",
       " 494: 'much',\n",
       " 495: 'words',\n",
       " 496: 'press',\n",
       " 497: 'energy',\n",
       " 498: 'think',\n",
       " 499: 'kitchen',\n",
       " 500: 'teams',\n",
       " 501: 'roseanne',\n",
       " 502: 'matter',\n",
       " 503: 'warm',\n",
       " 504: 'cohen',\n",
       " 505: 'eye',\n",
       " 506: 'wait',\n",
       " 507: 'seized',\n",
       " 508: 'civil',\n",
       " 509: 'brain',\n",
       " 510: 'die',\n",
       " 511: 'bad',\n",
       " 512: 'island',\n",
       " 513: 'target',\n",
       " 514: 'michael',\n",
       " 515: 'complicated',\n",
       " 516: 'under',\n",
       " 517: 'test',\n",
       " 518: 'did',\n",
       " 519: 'talks',\n",
       " 520: 'put',\n",
       " 521: 'questions',\n",
       " 522: 'dies',\n",
       " 523: 'moves',\n",
       " 524: '15',\n",
       " 525: 'lives',\n",
       " 526: 'james',\n",
       " 527: 'gives',\n",
       " 528: 'warriors',\n",
       " 529: 'mainstream',\n",
       " 530: 'saudi',\n",
       " 531: 'loan',\n",
       " 532: 'puts',\n",
       " 533: 'star',\n",
       " 534: 'broken',\n",
       " 535: 'glass',\n",
       " 536: 'moments',\n",
       " 537: 'sick',\n",
       " 538: 'beat',\n",
       " 539: 'god',\n",
       " 540: 'mean',\n",
       " 541: 'spy',\n",
       " 542: 'immigrants',\n",
       " 543: 'kill',\n",
       " 544: 'shame',\n",
       " 545: 'maybe',\n",
       " 546: 'even',\n",
       " 547: 'land',\n",
       " 548: '14',\n",
       " 549: 'policy',\n",
       " 550: 'agent',\n",
       " 551: 'smart',\n",
       " 552: 'martin',\n",
       " 553: 'luther',\n",
       " 554: 'mind',\n",
       " 555: 'earth',\n",
       " 556: 'action',\n",
       " 557: 'iraq',\n",
       " 558: 'feeling',\n",
       " 559: 'nature',\n",
       " 560: 'church',\n",
       " 561: 'beware',\n",
       " 562: 'away',\n",
       " 563: 'point',\n",
       " 564: 'army',\n",
       " 565: 'va',\n",
       " 566: 'girls',\n",
       " 567: 'play',\n",
       " 568: 'made',\n",
       " 569: 'marathon',\n",
       " 570: 'lies',\n",
       " 571: 'turn',\n",
       " 572: 'fiction',\n",
       " 573: 'terror',\n",
       " 574: 'accept',\n",
       " 575: 'dying',\n",
       " 576: 'victims',\n",
       " 577: 'golden',\n",
       " 578: 'common',\n",
       " 579: 'near',\n",
       " 580: 'cosby',\n",
       " 581: 'revolt',\n",
       " 582: 'offer',\n",
       " 583: 'rule',\n",
       " 584: 'bag',\n",
       " 585: 'tradition',\n",
       " 586: 'failed',\n",
       " 587: 'utah',\n",
       " 588: 'few',\n",
       " 589: 'believe',\n",
       " 590: 'forced',\n",
       " 591: 'artists',\n",
       " 592: 'carter',\n",
       " 593: 'jersey',\n",
       " 594: 'quiz',\n",
       " 595: 'lifethreatening',\n",
       " 596: 'food',\n",
       " 597: 'choosing',\n",
       " 598: 'future',\n",
       " 599: 'gone',\n",
       " 600: 'panel',\n",
       " 601: 'bucks',\n",
       " 602: 'sidewalk',\n",
       " 603: 'van',\n",
       " 604: 'cuts',\n",
       " 605: 'apologies',\n",
       " 606: 'europes',\n",
       " 607: 'iran',\n",
       " 608: 'themselves',\n",
       " 609: 'avengers',\n",
       " 610: 'most',\n",
       " 611: 'movie',\n",
       " 612: 'taking',\n",
       " 613: 'mirror',\n",
       " 614: 'chancellor',\n",
       " 615: 'debate',\n",
       " 616: 'foods',\n",
       " 617: 'citys',\n",
       " 618: 'favorite',\n",
       " 619: 'cruelty',\n",
       " 620: 'robots',\n",
       " 621: 'coffee',\n",
       " 622: 'prince',\n",
       " 623: 'share',\n",
       " 624: 'tolerance',\n",
       " 625: 'stop',\n",
       " 626: 'migrants',\n",
       " 627: 'path',\n",
       " 628: 'aiding',\n",
       " 629: 'europe',\n",
       " 630: 'magic',\n",
       " 631: 'kind',\n",
       " 632: 'losing',\n",
       " 633: 'middle',\n",
       " 634: 'class',\n",
       " 635: 'youll',\n",
       " 636: 'sorry',\n",
       " 637: 'swamp',\n",
       " 638: 'columbia',\n",
       " 639: 'wages',\n",
       " 640: 'painfully',\n",
       " 641: 'wonkish',\n",
       " 642: 'forgotten',\n",
       " 643: 'survival',\n",
       " 644: 'decline',\n",
       " 645: 'mess',\n",
       " 646: 'fake',\n",
       " 647: 'set',\n",
       " 648: 'himself',\n",
       " 649: 'barely',\n",
       " 650: 'quit',\n",
       " 651: '70',\n",
       " 652: 'conspiracy',\n",
       " 653: 'pick',\n",
       " 654: 'wells',\n",
       " 655: 'fargo',\n",
       " 656: 'mayhem',\n",
       " 657: 'fields',\n",
       " 658: '50',\n",
       " 659: 'lack',\n",
       " 660: 'cynthia',\n",
       " 661: 'nixon',\n",
       " 662: 'along',\n",
       " 663: 'promise',\n",
       " 664: 'testing',\n",
       " 665: 'candidates',\n",
       " 666: 'pruitts',\n",
       " 667: 'reforms',\n",
       " 668: 'selling',\n",
       " 669: 'know',\n",
       " 670: 'runners',\n",
       " 671: 'tarot',\n",
       " 672: 'card',\n",
       " 673: 'dress',\n",
       " 674: 'pilot',\n",
       " 675: 'nerves',\n",
       " 676: 'letting',\n",
       " 677: 'al',\n",
       " 678: 'equal',\n",
       " 679: 'returns',\n",
       " 680: 'snake',\n",
       " 681: 'oil',\n",
       " 682: 'betrayed',\n",
       " 683: 'nation',\n",
       " 684: 'fashion',\n",
       " 685: 'lay',\n",
       " 686: 'fine',\n",
       " 687: 'fraud',\n",
       " 688: 'again',\n",
       " 689: 'exfbi',\n",
       " 690: 'sent',\n",
       " 691: 'name',\n",
       " 692: 'sea',\n",
       " 693: 'stands',\n",
       " 694: 'spur',\n",
       " 695: 'means',\n",
       " 696: 'progressive',\n",
       " 697: 'standardized',\n",
       " 698: 'tests',\n",
       " 699: 'antibias',\n",
       " 700: 'goal',\n",
       " 701: 'gentrification',\n",
       " 702: 'shine',\n",
       " 703: 'road',\n",
       " 704: 'deportation',\n",
       " 705: 'voice',\n",
       " 706: 'crazy',\n",
       " 707: 'warming',\n",
       " 708: 'interview',\n",
       " 709: 'launches',\n",
       " 710: 'allout',\n",
       " 711: 'barbara',\n",
       " 712: 'ill',\n",
       " 713: 'treatment',\n",
       " 714: 'blasts',\n",
       " 715: 'slippery',\n",
       " 716: 'around',\n",
       " 717: 'office',\n",
       " 718: 'todo',\n",
       " 719: 'list',\n",
       " 720: 'housing',\n",
       " 721: 'leaves',\n",
       " 722: 'pinch',\n",
       " 723: 'took',\n",
       " 724: 'assad',\n",
       " 725: 'library',\n",
       " 726: 'payment',\n",
       " 727: 'snarl',\n",
       " 728: 'makes',\n",
       " 729: '87',\n",
       " 730: 'comedy',\n",
       " 731: 'rising',\n",
       " 732: 'stars',\n",
       " 733: 'alike',\n",
       " 734: 'coal',\n",
       " 735: 'lobbyist',\n",
       " 736: 'crossword',\n",
       " 737: 'animals',\n",
       " 738: 'whos',\n",
       " 739: 'pentagon',\n",
       " 740: 'signs',\n",
       " 741: 'childhood',\n",
       " 742: 'courts',\n",
       " 743: 'shift',\n",
       " 744: 'saying',\n",
       " 745: 'scandal',\n",
       " 746: 'sticker',\n",
       " 747: 'shock',\n",
       " 748: 'teacher',\n",
       " 749: 'walkouts',\n",
       " 750: 'grip',\n",
       " 751: 'red',\n",
       " 752: 'speaker',\n",
       " 753: 'leave',\n",
       " 754: 'british',\n",
       " 755: 'mexico',\n",
       " 756: 'wish',\n",
       " 757: 'alive',\n",
       " 758: 'longer',\n",
       " 759: 'moscow',\n",
       " 760: 'believes',\n",
       " 761: 'beef',\n",
       " 762: 'stew',\n",
       " 763: 'chinese',\n",
       " 764: 'partner',\n",
       " 765: 'tabloid',\n",
       " 766: 'catches',\n",
       " 767: 'facebooks',\n",
       " 768: 'lot',\n",
       " 769: 'health',\n",
       " 770: 'putin',\n",
       " 771: 'india',\n",
       " 772: 'these',\n",
       " 773: 'begin',\n",
       " 774: 'protest',\n",
       " 775: 'florida',\n",
       " 776: 'colorado',\n",
       " 777: 'thats',\n",
       " 778: 'hopes',\n",
       " 779: 'access',\n",
       " 780: 'hollywood',\n",
       " 781: 'tape',\n",
       " 782: 'focus',\n",
       " 783: 'fbi',\n",
       " 784: 'general',\n",
       " 785: 'leak',\n",
       " 786: 'spring',\n",
       " 787: 'gut',\n",
       " 788: 'advice',\n",
       " 789: 'risotto',\n",
       " 790: 'simple',\n",
       " 791: 'during',\n",
       " 792: 'week',\n",
       " 793: 'ball',\n",
       " 794: 'gluten',\n",
       " 795: 'free',\n",
       " 796: 'seattle',\n",
       " 797: 'exhusband',\n",
       " 798: 'dilemma',\n",
       " 799: 'betting',\n",
       " 800: 'governors',\n",
       " 801: 'collector',\n",
       " 802: 'thriving',\n",
       " 803: 'modern',\n",
       " 804: 'count',\n",
       " 805: 'thousands',\n",
       " 806: 'interest',\n",
       " 807: 'alaska',\n",
       " 808: 'republican',\n",
       " 809: 'midterm',\n",
       " 810: 'elections',\n",
       " 811: 'pose',\n",
       " 812: 'worry',\n",
       " 813: 'view',\n",
       " 814: 'fascism',\n",
       " 815: 'without',\n",
       " 816: 'spending',\n",
       " 817: 'signals',\n",
       " 818: 'boys',\n",
       " 819: 'learning',\n",
       " 820: 'move',\n",
       " 821: 'fast',\n",
       " 822: 'governor',\n",
       " 823: 'those',\n",
       " 824: 'aging',\n",
       " 825: 'warns',\n",
       " 826: 'media',\n",
       " 827: 'exercise',\n",
       " 828: 'allergic',\n",
       " 829: 'signatures',\n",
       " 830: 'military',\n",
       " 831: 'disaster',\n",
       " 832: 'embrace',\n",
       " 833: 'youre',\n",
       " 834: 'defying',\n",
       " 835: 'toll',\n",
       " 836: 'humans',\n",
       " 837: 'store',\n",
       " 838: 'dogs',\n",
       " 839: 'client',\n",
       " 840: 'reports',\n",
       " 841: 'fired',\n",
       " 842: 'coming',\n",
       " 843: 'lavish',\n",
       " 844: 'zone',\n",
       " 845: 'called',\n",
       " 846: 'options',\n",
       " 847: 'paid',\n",
       " 848: 'ice',\n",
       " 849: 'adviser',\n",
       " 850: 'calls',\n",
       " 851: 'polar',\n",
       " 852: 'bears',\n",
       " 853: 'online',\n",
       " 854: 'bust',\n",
       " 855: 'bars',\n",
       " 856: 'private',\n",
       " 857: 'fans',\n",
       " 858: 'remains',\n",
       " 859: 'funny',\n",
       " 860: 'daughter',\n",
       " 861: 'offered',\n",
       " 862: 'massacre',\n",
       " 863: 'prison',\n",
       " 864: 'votes',\n",
       " 865: 'very',\n",
       " 866: 'outsiders',\n",
       " 867: 'gender',\n",
       " 868: 'scrutiny',\n",
       " 869: 'trail',\n",
       " 870: 'nightmare',\n",
       " 871: 'species',\n",
       " 872: 'fuels',\n",
       " 873: 'jacket',\n",
       " 874: 'hair',\n",
       " 875: '81',\n",
       " 876: 'foreign',\n",
       " 877: '9',\n",
       " 878: 'arabian',\n",
       " 879: 'official',\n",
       " 880: 'afghan',\n",
       " 881: 'enters',\n",
       " 882: 'journalism',\n",
       " 883: 'later',\n",
       " 884: 'cry',\n",
       " 885: 'vows',\n",
       " 886: 'price',\n",
       " 887: 'raising',\n",
       " 888: 'song',\n",
       " 889: 'gallery',\n",
       " 890: 'hockey',\n",
       " 891: 'fox',\n",
       " 892: 'fiery',\n",
       " 893: 'chemical',\n",
       " 894: 'hasty',\n",
       " 895: 'viral',\n",
       " 896: 'voters',\n",
       " 897: 'arlee',\n",
       " 898: 'arent',\n",
       " 899: 'friday',\n",
       " 900: 'something',\n",
       " 901: 'every',\n",
       " 902: 'buying',\n",
       " 903: 'ad',\n",
       " 904: 'asparagus',\n",
       " 905: 'blame',\n",
       " 906: 'democracy',\n",
       " 907: 'bracing',\n",
       " 908: 'blood',\n",
       " 909: 'estate',\n",
       " 910: 'jobs',\n",
       " 911: 'raised',\n",
       " 912: 'islam',\n",
       " 913: 'yorks',\n",
       " 914: 'give',\n",
       " 915: 'blast',\n",
       " 916: 'small',\n",
       " 917: 'forces',\n",
       " 918: 'tight',\n",
       " 919: 'labor',\n",
       " 920: 'manhattan',\n",
       " 921: 'intellectual',\n",
       " 922: 'tiny',\n",
       " 923: 'haunting',\n",
       " 924: 'thoughts',\n",
       " 925: 'misconduct',\n",
       " 926: 'usual',\n",
       " 927: 'lisa',\n",
       " 928: 'evil',\n",
       " 929: 'springs',\n",
       " 930: 'stress',\n",
       " 931: 'monkeys',\n",
       " 932: 'gay',\n",
       " 933: 'safety',\n",
       " 934: 'crashes',\n",
       " 935: 'queen',\n",
       " 936: 'indexes',\n",
       " 937: 'indian',\n",
       " 938: 'dreams',\n",
       " 939: 'role',\n",
       " 940: 'macrons',\n",
       " 941: 'locals',\n",
       " 942: 'shot',\n",
       " 943: 'whims',\n",
       " 944: 'sees',\n",
       " 945: 'makers',\n",
       " 946: 'pipe',\n",
       " 947: 'slow',\n",
       " 948: 'quiet',\n",
       " 949: 'ohio',\n",
       " 950: 'primary',\n",
       " 951: 'readers',\n",
       " 952: 'universe',\n",
       " 953: 'older',\n",
       " 954: 'well',\n",
       " 955: 'rents',\n",
       " 956: 'saving',\n",
       " 957: 'worship',\n",
       " 958: 'detail',\n",
       " 959: 'project',\n",
       " 960: 'dissent',\n",
       " 961: 'kennedy',\n",
       " 962: 'assassination',\n",
       " 963: 'edges',\n",
       " 964: 'brazil',\n",
       " 965: 'sports',\n",
       " 966: 'dad',\n",
       " 967: 'amazon',\n",
       " 968: 'trauma',\n",
       " 969: 'anxiety',\n",
       " 970: '100',\n",
       " 971: 'endless',\n",
       " 972: 'video',\n",
       " 973: 'legacy',\n",
       " 974: 'candidate',\n",
       " 975: 'shooting',\n",
       " 976: 'silicon',\n",
       " 977: 'valley',\n",
       " 978: 'light',\n",
       " 979: 'chose',\n",
       " 980: 'heated',\n",
       " 981: 'leads',\n",
       " 982: 'israelis',\n",
       " 983: 'lone',\n",
       " 984: 'journalist',\n",
       " 985: 'sexual',\n",
       " 986: 'assault',\n",
       " 987: 'joke',\n",
       " 988: 'bronx',\n",
       " 989: 'necessary',\n",
       " 990: 'search',\n",
       " 991: 'nights',\n",
       " 992: 'giants',\n",
       " 993: 'streets',\n",
       " 994: 'stocks',\n",
       " 995: 'see',\n",
       " 996: 'teachers',\n",
       " 997: 'spreads',\n",
       " 998: 'african',\n",
       " 999: 'only',\n",
       " 1000: 'hard',\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = {}\n",
    "for key, value in tokenizer.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key\n",
    "\n",
    "index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13925d",
   "metadata": {},
   "source": [
    "### 패딩"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f66e33a9",
   "metadata": {},
   "source": [
    "이제 전체 샘플의 길이를 동일하게 만드는 패딩 작업을 수행합니다. 패딩 작업을 수행하기 전에 가장 긴 샘플의 길이를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e0be2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 24\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in sequences)\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f25250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0   99  269]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0   99  269  371]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0   99  269  371 1115]]\n"
     ]
    }
   ],
   "source": [
    "# 가장 긴 샘플의 길이인 24로 모든 샘플의 길이를 패딩하겠습니다.\n",
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences[:3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c4a8ec7",
   "metadata": {},
   "source": [
    "이제 피쳐와 레이블로 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff8f114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = sequences[:, :-1]  # 피쳐\n",
    "y = sequences[:, -1]   # 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb9f59cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0  99]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0  99 269]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  99 269 371]]\n",
      "[ 269  371 1115]\n"
     ]
    }
   ],
   "source": [
    "print(X[:3])\n",
    "print(y[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f285761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 레이블 데이터 y에 대하여 원-핫 인코딩 수행\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf8399",
   "metadata": {},
   "source": [
    "## 2) 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ea4040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b7379f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3494"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd391ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=23))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7585ca31",
   "metadata": {},
   "source": [
    "하이퍼파라미터인 임베딩 벡터의 차원은 10, 은닉 상태의 크기는 128입니다. \n",
    "\n",
    "다 대 일 구조의 LSTM을 사용합니다. \n",
    "\n",
    "전결합층(Fully Connected Layer)을 출력층으로 단어 집합 크기만큼의 뉴런을 배치하여 모델을 설계합니다. 해당 모델은 마지막 시점에서 모든 가능한 단어 중 하나의 단어를 예측하는 다중 클래스 분류 문제를 수행하는 모델입니다. 다중 클래스 분류 문제의 경우, 출력층에 소프트맥스 회귀를 사용해야 하므로 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로 크로스 엔트로피 함수를 사용하여 200 에포크를 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c7bf00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "244/244 - 9s - loss: 7.6457 - accuracy: 0.0277 - 9s/epoch - 36ms/step\n",
      "Epoch 2/50\n",
      "244/244 - 6s - loss: 7.1128 - accuracy: 0.0300 - 6s/epoch - 26ms/step\n",
      "Epoch 3/50\n",
      "244/244 - 6s - loss: 6.9768 - accuracy: 0.0329 - 6s/epoch - 25ms/step\n",
      "Epoch 4/50\n",
      "244/244 - 6s - loss: 6.8572 - accuracy: 0.0402 - 6s/epoch - 26ms/step\n",
      "Epoch 5/50\n",
      "244/244 - 7s - loss: 6.7124 - accuracy: 0.0423 - 7s/epoch - 27ms/step\n",
      "Epoch 6/50\n",
      "244/244 - 6s - loss: 6.5451 - accuracy: 0.0474 - 6s/epoch - 24ms/step\n",
      "Epoch 7/50\n",
      "244/244 - 6s - loss: 6.3660 - accuracy: 0.0513 - 6s/epoch - 25ms/step\n",
      "Epoch 8/50\n",
      "244/244 - 7s - loss: 6.1718 - accuracy: 0.0577 - 7s/epoch - 28ms/step\n",
      "Epoch 9/50\n",
      "244/244 - 6s - loss: 5.9863 - accuracy: 0.0606 - 6s/epoch - 25ms/step\n",
      "Epoch 10/50\n",
      "244/244 - 7s - loss: 5.8047 - accuracy: 0.0668 - 7s/epoch - 27ms/step\n",
      "Epoch 11/50\n",
      "244/244 - 7s - loss: 5.6317 - accuracy: 0.0697 - 7s/epoch - 28ms/step\n",
      "Epoch 12/50\n",
      "244/244 - 7s - loss: 5.4703 - accuracy: 0.0754 - 7s/epoch - 27ms/step\n",
      "Epoch 13/50\n",
      "244/244 - 7s - loss: 5.3170 - accuracy: 0.0777 - 7s/epoch - 29ms/step\n",
      "Epoch 14/50\n",
      "244/244 - 8s - loss: 5.1701 - accuracy: 0.0864 - 8s/epoch - 32ms/step\n",
      "Epoch 15/50\n",
      "244/244 - 7s - loss: 5.0255 - accuracy: 0.0952 - 7s/epoch - 30ms/step\n",
      "Epoch 16/50\n",
      "244/244 - 7s - loss: 4.8897 - accuracy: 0.1018 - 7s/epoch - 28ms/step\n",
      "Epoch 17/50\n",
      "244/244 - 6s - loss: 4.7591 - accuracy: 0.1139 - 6s/epoch - 26ms/step\n",
      "Epoch 18/50\n",
      "244/244 - 7s - loss: 4.6314 - accuracy: 0.1274 - 7s/epoch - 28ms/step\n",
      "Epoch 19/50\n",
      "244/244 - 7s - loss: 4.5059 - accuracy: 0.1424 - 7s/epoch - 27ms/step\n",
      "Epoch 20/50\n",
      "244/244 - 6s - loss: 4.3859 - accuracy: 0.1560 - 6s/epoch - 27ms/step\n",
      "Epoch 21/50\n",
      "244/244 - 7s - loss: 4.2689 - accuracy: 0.1733 - 7s/epoch - 28ms/step\n",
      "Epoch 22/50\n",
      "244/244 - 8s - loss: 4.1536 - accuracy: 0.1915 - 8s/epoch - 31ms/step\n",
      "Epoch 23/50\n",
      "244/244 - 8s - loss: 4.0441 - accuracy: 0.2044 - 8s/epoch - 31ms/step\n",
      "Epoch 24/50\n",
      "244/244 - 7s - loss: 3.9371 - accuracy: 0.2239 - 7s/epoch - 29ms/step\n",
      "Epoch 25/50\n",
      "244/244 - 6s - loss: 3.8332 - accuracy: 0.2373 - 6s/epoch - 26ms/step\n",
      "Epoch 26/50\n",
      "244/244 - 7s - loss: 3.7342 - accuracy: 0.2580 - 7s/epoch - 27ms/step\n",
      "Epoch 27/50\n",
      "244/244 - 7s - loss: 3.6344 - accuracy: 0.2781 - 7s/epoch - 30ms/step\n",
      "Epoch 28/50\n",
      "244/244 - 8s - loss: 3.5406 - accuracy: 0.2910 - 8s/epoch - 32ms/step\n",
      "Epoch 29/50\n",
      "244/244 - 8s - loss: 3.4488 - accuracy: 0.3130 - 8s/epoch - 33ms/step\n",
      "Epoch 30/50\n",
      "244/244 - 7s - loss: 3.3621 - accuracy: 0.3260 - 7s/epoch - 30ms/step\n",
      "Epoch 31/50\n",
      "244/244 - 7s - loss: 3.2771 - accuracy: 0.3388 - 7s/epoch - 27ms/step\n",
      "Epoch 32/50\n",
      "244/244 - 6s - loss: 3.1901 - accuracy: 0.3585 - 6s/epoch - 26ms/step\n",
      "Epoch 33/50\n",
      "244/244 - 6s - loss: 3.1134 - accuracy: 0.3668 - 6s/epoch - 26ms/step\n",
      "Epoch 34/50\n",
      "244/244 - 6s - loss: 3.0363 - accuracy: 0.3824 - 6s/epoch - 25ms/step\n",
      "Epoch 35/50\n",
      "244/244 - 6s - loss: 2.9626 - accuracy: 0.3979 - 6s/epoch - 25ms/step\n",
      "Epoch 36/50\n",
      "244/244 - 6s - loss: 2.8901 - accuracy: 0.4133 - 6s/epoch - 25ms/step\n",
      "Epoch 37/50\n",
      "244/244 - 6s - loss: 2.8195 - accuracy: 0.4243 - 6s/epoch - 25ms/step\n",
      "Epoch 38/50\n",
      "244/244 - 6s - loss: 2.7533 - accuracy: 0.4364 - 6s/epoch - 25ms/step\n",
      "Epoch 39/50\n",
      "244/244 - 6s - loss: 2.6893 - accuracy: 0.4502 - 6s/epoch - 25ms/step\n",
      "Epoch 40/50\n",
      "244/244 - 6s - loss: 2.6255 - accuracy: 0.4598 - 6s/epoch - 26ms/step\n",
      "Epoch 41/50\n",
      "244/244 - 6s - loss: 2.5662 - accuracy: 0.4774 - 6s/epoch - 26ms/step\n",
      "Epoch 42/50\n",
      "244/244 - 6s - loss: 2.5071 - accuracy: 0.4821 - 6s/epoch - 26ms/step\n",
      "Epoch 43/50\n",
      "244/244 - 6s - loss: 2.4499 - accuracy: 0.4972 - 6s/epoch - 26ms/step\n",
      "Epoch 44/50\n",
      "244/244 - 6s - loss: 2.3920 - accuracy: 0.5078 - 6s/epoch - 25ms/step\n",
      "Epoch 45/50\n",
      "244/244 - 6s - loss: 2.3371 - accuracy: 0.5202 - 6s/epoch - 25ms/step\n",
      "Epoch 46/50\n",
      "244/244 - 6s - loss: 2.2861 - accuracy: 0.5327 - 6s/epoch - 25ms/step\n",
      "Epoch 47/50\n",
      "244/244 - 7s - loss: 2.2333 - accuracy: 0.5375 - 7s/epoch - 27ms/step\n",
      "Epoch 48/50\n",
      "244/244 - 6s - loss: 2.1843 - accuracy: 0.5489 - 6s/epoch - 25ms/step\n",
      "Epoch 49/50\n",
      "244/244 - 6s - loss: 2.1345 - accuracy: 0.5576 - 6s/epoch - 25ms/step\n",
      "Epoch 50/50\n",
      "244/244 - 6s - loss: 2.0835 - accuracy: 0.5693 - 6s/epoch - 26ms/step\n",
      "CPU times: user 12min 21s, sys: 6min 36s, total: 18min 58s\n",
      "Wall time: 5min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb670ecb640>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(X, y, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f07d52b",
   "metadata": {},
   "source": [
    "문장을 생성하는 함수 sentence_generation을 만들어서 문장을 생성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56e881db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "\n",
    "    # n번 반복\n",
    "    for _ in range(n):\n",
    "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen=max_len-1, padding='pre')\n",
    "\n",
    "        # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        result = model.predict(encoded, verbose=0)\n",
    "        result = np.argmax(result, axis=1)\n",
    "\n",
    "        for word, index in tokenizer.word_index.items(): \n",
    "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "            if index == result:\n",
    "                break\n",
    "\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        current_word = current_word + ' '  + word\n",
    "\n",
    "        # 예측 단어를 문장에 저장\n",
    "        sentence = sentence + ' ' + word\n",
    "\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae3fe37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i going on trade baffle chinas leaders say learn for his\n"
     ]
    }
   ],
   "source": [
    "# 임의의 단어 'i'에 대해서 10개의 단어를 추가 생성해봅시다.\n",
    "print(sentence_generation(model, tokenizer, 'i', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9193d581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how a subject bridge and sensible the elderly need not apply\n"
     ]
    }
   ],
   "source": [
    "# 임의의 단어 'how'에 대해서 10개의 단어를 추가 생성해봅시다.\n",
    "print(sentence_generation(model, tokenizer, 'how', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d534faa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe640ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
